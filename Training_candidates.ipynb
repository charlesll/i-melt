{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8238,
     "status": "ok",
     "timestamp": 1565502147863,
     "user": {
      "displayName": "Charles LL",
      "photoUrl": "",
      "userId": "05619559028255365437"
     },
     "user_tz": -600
    },
    "id": "e0SePNd0dLJc",
    "outputId": "03c95576-8b80-4218-bfab-11d7eed6740e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA AVAILABLE?  True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd # manipulate dataframes\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np\n",
    "np.random.seed = 167 # fix random seed for reproducibility\n",
    "import time, h5py, imelt, torch\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "# importing shutil module  \n",
    "import shutil \n",
    "\n",
    "# First we check if CUDA is available\n",
    "print(\"CUDA AVAILABLE? \",torch.cuda.is_available())\n",
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "      \n",
    "device = get_default_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "#### Dataset specifications ####\n",
      "################################\n",
      "\n",
      "Number of unique compositions (viscosity): 173\n",
      "Number of unique compositions in training (viscosity): 113\n",
      "Dataset separations are 0.65 in train, 0.14 in valid, 0.21 in test\n",
      "\n",
      "Number of unique compositions (entropy): 44\n",
      "Number of unique compositions in training (entropy): 30\n",
      "Dataset separations are 0.68 in train, 0.16 in valid, 0.16 in test\n",
      "\n",
      "Number of unique compositions (refractive index): 243\n",
      "Number of unique compositions in training (refractive index): 167\n",
      "Dataset separations are 0.69 in train, 0.16 in valid, 0.16 in test\n",
      "\n",
      "Number of unique compositions (density): 205\n",
      "Number of unique compositions in training (density): 139\n",
      "Dataset separations are 0.68 in train, 0.16 in valid, 0.16 in test\n",
      "\n",
      "Number of unique compositions (Raman): 58\n",
      "Number of unique compositions in training (Raman): 48\n",
      "Dataset separations are 0.83 in train, 0.17 in valid\n",
      "\n",
      "This is for checking the consistency of the dataset...\n",
      "Visco train shape\n",
      "torch.Size([1198, 4])\n",
      "torch.Size([1198, 1])\n",
      "torch.Size([1198, 1])\n",
      "Entropy train shape\n",
      "torch.Size([30, 4])\n",
      "torch.Size([30, 1])\n",
      "Tg train shape\n",
      "torch.Size([678, 4])\n",
      "torch.Size([678, 1])\n",
      "Density train shape\n",
      "torch.Size([154, 4])\n",
      "torch.Size([154, 1])\n",
      "Refactive Index train shape\n",
      "torch.Size([238, 4])\n",
      "torch.Size([238, 1])\n",
      "torch.Size([238, 1])\n",
      "Raman train shape\n",
      "torch.Size([52, 4])\n",
      "torch.Size([52, 850])\n",
      "\n",
      "Where are the datasets? CPU or GPU?\n",
      "Visco device\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "Entropy device\n",
      "cuda:0\n",
      "cuda:0\n",
      "Tg device\n",
      "cuda:0\n",
      "cuda:0\n",
      "Density device\n",
      "cuda:0\n",
      "cuda:0\n",
      "Refactive Index device\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "Raman device\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# custom data loader, automatically sent to device\n",
    "ds = imelt.data_loader(\"./data/NKAS_viscosity_reference.hdf5\",\n",
    "                         \"./data/NKAS_Raman.hdf5\",\n",
    "                         \"./data/NKAS_density.hdf5\",\n",
    "                         \"./data/NKAS_optical.hdf5\",\n",
    "                         device)\n",
    "\n",
    "ds.print_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 1 model \n",
    "\n",
    "Selected architecture: 4 layers, 200 neurons per layer, very low dropout\n",
    "\n",
    "Parameters were tuned after the random search, & learning rate by Bayesian Optimization & patience by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model(\n",
       "  (activation_function): ReLU()\n",
       "  (dropout): Dropout(p=0.01, inplace=False)\n",
       "  (linears): ModuleList(\n",
       "    (0): Linear(in_features=4, out_features=200, bias=True)\n",
       "    (1): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (3): Linear(in_features=200, out_features=200, bias=True)\n",
       "  )\n",
       "  (out_thermo): Linear(in_features=200, out_features=17, bias=True)\n",
       "  (out_raman): Linear(in_features=200, out_features=850, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_layers = 4\n",
    "nb_neurons = 200\n",
    "p_drop = 0.01\n",
    "\n",
    "name = \"./model/candidates/l\"+str(nb_layers)+\"_n\"+str(nb_neurons)+\"_p\"+str(p_drop)+\"_test\"+\".pth\"\n",
    "\n",
    "# declaring model\n",
    "neuralmodel = imelt.model(4,nb_neurons,nb_layers,ds.nb_channels_raman,p_drop=p_drop) \n",
    "\n",
    "# criterion for match\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "criterion.to(device) # sending criterion on device\n",
    "\n",
    "# we initialize the output bias and send the neural net on device\n",
    "neuralmodel.output_bias_init()\n",
    "neuralmodel = neuralmodel.float()\n",
    "neuralmodel.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 195.1467742919922; valid loss: 143.09413146972656\n",
      "Epoch 200 => train loss: 5.904855251312256; valid loss: 10.56926441192627\n",
      "Epoch 400 => train loss: 3.0956780910491943; valid loss: 4.933417320251465\n",
      "Epoch 600 => train loss: 2.0335700511932373; valid loss: 3.4598968029022217\n",
      "Epoch 800 => train loss: 1.6537256240844727; valid loss: 2.89522647857666\n",
      "Epoch 1000 => train loss: 1.4856466054916382; valid loss: 2.758596897125244\n",
      "Epoch 1200 => train loss: 1.2512149810791016; valid loss: 2.335705041885376\n",
      "Epoch 1400 => train loss: 1.146148681640625; valid loss: 2.6345224380493164\n",
      "Epoch 1600 => train loss: 1.1321202516555786; valid loss: 2.3829894065856934\n",
      "Running time in seconds: 117.333811044693\n",
      "Scaled valid loss values are 0.89, 0.19, 0.53, 0.70, 0.40, 0.39 for Tg, Raman, density, entropy, ri, viscosity (AG)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# TRAINING\n",
    "#\n",
    "optimizer = torch.optim.Adam(neuralmodel.parameters(), lr = 0.0006) # optimizer\n",
    "neuralmodel, record_train_loss, record_valid_loss = imelt.training(neuralmodel,ds,\n",
    "                                                                     criterion,optimizer,save_switch=True,save_name=name,\n",
    "                                                                     train_patience=400,min_delta=0.05,\n",
    "                                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observing how losses decline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/candidates/l4_n200_p0.01_test.pth\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(name)\n",
    "plt.figure()\n",
    "plt.plot(record_train_loss,label=\"train\")\n",
    "plt.plot(record_valid_loss,label=\"valid\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f34a075b700>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([5,14],[5,14],'k--')\n",
    "plt.plot(neuralmodel.sctg(ds.x_entro_valid).cpu().detach().numpy(),ds.y_entro_valid.cpu().detach().numpy(),'ks')\n",
    "plt.plot(neuralmodel.sctg(ds.x_entro_train).cpu().detach().numpy(),ds.y_entro_train.cpu().detach().numpy(),'rs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43276978\n",
      "0.42483526\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(mean_squared_error(neuralmodel.ag(ds.x_visco_train,ds.T_visco_train).cpu().detach().numpy(), ds.y_visco_train.cpu().detach().numpy(),squared=False))\n",
    "print(mean_squared_error(neuralmodel.ag(ds.x_visco_valid,ds.T_visco_valid).cpu().detach().numpy(), ds.y_visco_valid.cpu().detach().numpy(),squared=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 50 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 179.16818237304688; valid loss: 141.85609436035156\n",
      "Epoch 200 => train loss: 2.788405656814575; valid loss: 6.687092304229736\n",
      "Epoch 400 => train loss: 1.6745893955230713; valid loss: 2.9017691612243652\n",
      "Epoch 600 => train loss: 1.2475261688232422; valid loss: 2.233450412750244\n",
      "Epoch 800 => train loss: 1.0846185684204102; valid loss: 2.4305827617645264\n",
      "Epoch 1000 => train loss: 0.9899698495864868; valid loss: 2.4072437286376953\n",
      "Epoch 1200 => train loss: 0.8483244180679321; valid loss: 2.519680976867676\n",
      "Running time in seconds: 86.92949557304382\n",
      "Scaled valid loss values are 1.04, 0.21, 0.35, 0.85, 0.28, 0.17 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 220.70530700683594; valid loss: 172.99798583984375\n",
      "Epoch 200 => train loss: 2.9240972995758057; valid loss: 6.13416862487793\n",
      "Epoch 400 => train loss: 1.6997066736221313; valid loss: 2.6970012187957764\n",
      "Epoch 600 => train loss: 1.2407851219177246; valid loss: 2.418107271194458\n",
      "Epoch 800 => train loss: 1.1880546808242798; valid loss: 2.5989480018615723\n",
      "Epoch 1000 => train loss: 0.978630542755127; valid loss: 2.5843732357025146\n",
      "Epoch 1200 => train loss: 0.8274912238121033; valid loss: 2.6606757640838623\n",
      "Running time in seconds: 83.87028074264526\n",
      "Scaled valid loss values are 1.17, 0.17, 0.47, 0.89, 0.28, 0.20 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 193.40664672851562; valid loss: 156.348388671875\n",
      "Epoch 200 => train loss: 3.1549808979034424; valid loss: 6.102735996246338\n",
      "Epoch 400 => train loss: 1.6670515537261963; valid loss: 2.649068593978882\n",
      "Epoch 600 => train loss: 1.3099026679992676; valid loss: 2.638115644454956\n",
      "Epoch 800 => train loss: 1.162602424621582; valid loss: 2.685781955718994\n",
      "Epoch 1000 => train loss: 0.9603896141052246; valid loss: 2.6819705963134766\n",
      "Running time in seconds: 72.50128746032715\n",
      "Scaled valid loss values are 1.08, 0.23, 0.47, 0.74, 0.25, 0.18 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 208.96307373046875; valid loss: 165.19630432128906\n",
      "Epoch 200 => train loss: 3.332879066467285; valid loss: 6.913987636566162\n",
      "Epoch 400 => train loss: 1.9149051904678345; valid loss: 3.111816167831421\n",
      "Epoch 600 => train loss: 1.3282115459442139; valid loss: 2.5239343643188477\n",
      "Epoch 800 => train loss: 1.1412949562072754; valid loss: 2.741051435470581\n",
      "Epoch 1000 => train loss: 1.0386122465133667; valid loss: 2.436976194381714\n",
      "Running time in seconds: 73.89790558815002\n",
      "Scaled valid loss values are 1.31, 0.21, 0.48, 0.80, 0.42, 0.23 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 158.63804626464844; valid loss: 119.17481994628906\n",
      "Epoch 200 => train loss: 3.465747594833374; valid loss: 6.47774076461792\n",
      "Epoch 400 => train loss: 1.8203349113464355; valid loss: 2.908430814743042\n",
      "Epoch 600 => train loss: 1.3373831510543823; valid loss: 2.5393474102020264\n",
      "Epoch 800 => train loss: 1.2060390710830688; valid loss: 2.931049108505249\n",
      "Epoch 1000 => train loss: 1.0382729768753052; valid loss: 2.6105268001556396\n",
      "Running time in seconds: 70.36902713775635\n",
      "Scaled valid loss values are 1.15, 0.21, 0.52, 0.62, 0.27, 0.21 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 157.27308654785156; valid loss: 117.91497039794922\n",
      "Epoch 200 => train loss: 2.9361116886138916; valid loss: 4.9514241218566895\n",
      "Epoch 400 => train loss: 1.5921134948730469; valid loss: 3.198751211166382\n",
      "Epoch 600 => train loss: 1.357622742652893; valid loss: 2.9101481437683105\n",
      "Epoch 800 => train loss: 1.0509727001190186; valid loss: 3.1191303730010986\n",
      "Running time in seconds: 64.36671495437622\n",
      "Scaled valid loss values are 1.21, 0.19, 0.63, 1.40, 0.23, 0.19 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 192.61614990234375; valid loss: 152.37303161621094\n",
      "Epoch 200 => train loss: 2.966621160507202; valid loss: 7.1285624504089355\n",
      "Epoch 400 => train loss: 1.5662097930908203; valid loss: 3.42102313041687\n",
      "Epoch 600 => train loss: 1.2724907398223877; valid loss: 2.2417829036712646\n",
      "Epoch 800 => train loss: 1.0718176364898682; valid loss: 2.505082607269287\n",
      "Running time in seconds: 67.59116196632385\n",
      "Scaled valid loss values are 1.20, 0.18, 0.62, 0.65, 0.21, 0.23 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 176.8299102783203; valid loss: 143.7279815673828\n",
      "Epoch 200 => train loss: 3.293562889099121; valid loss: 6.4929518699646\n",
      "Epoch 400 => train loss: 1.8587672710418701; valid loss: 3.2849137783050537\n",
      "Epoch 600 => train loss: 1.4251364469528198; valid loss: 2.584627389907837\n",
      "Epoch 800 => train loss: 1.210896611213684; valid loss: 3.068809747695923\n",
      "Epoch 1000 => train loss: 1.0450011491775513; valid loss: 2.876086950302124\n",
      "Running time in seconds: 69.15401029586792\n",
      "Scaled valid loss values are 1.24, 0.21, 0.49, 1.11, 0.39, 0.20 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 204.06106567382812; valid loss: 170.58099365234375\n",
      "Epoch 200 => train loss: 3.1767308712005615; valid loss: 6.599137306213379\n",
      "Epoch 400 => train loss: 1.8190463781356812; valid loss: 3.3809919357299805\n",
      "Epoch 600 => train loss: 1.3641701936721802; valid loss: 2.666100025177002\n",
      "Epoch 800 => train loss: 1.1301215887069702; valid loss: 3.197746753692627\n",
      "Running time in seconds: 63.86976647377014\n",
      "Scaled valid loss values are 1.04, 0.21, 0.45, 1.65, 0.30, 0.21 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 167.72706604003906; valid loss: 130.59420776367188\n",
      "Epoch 200 => train loss: 3.188523769378662; valid loss: 7.399848461151123\n",
      "Epoch 400 => train loss: 1.8154029846191406; valid loss: 3.583191394805908\n",
      "Epoch 600 => train loss: 1.2713552713394165; valid loss: 2.6346683502197266\n",
      "Epoch 800 => train loss: 1.088808536529541; valid loss: 2.2986700534820557\n",
      "Epoch 1000 => train loss: 0.9878104329109192; valid loss: 2.2489235401153564\n",
      "Running time in seconds: 74.43036007881165\n",
      "Scaled valid loss values are 1.01, 0.21, 0.32, 0.60, 0.29, 0.18 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 183.62811279296875; valid loss: 145.88479614257812\n",
      "Epoch 200 => train loss: 3.1195967197418213; valid loss: 6.8044562339782715\n",
      "Epoch 400 => train loss: 1.6803373098373413; valid loss: 2.8967747688293457\n",
      "Epoch 600 => train loss: 1.3539314270019531; valid loss: 2.6394214630126953\n",
      "Epoch 800 => train loss: 1.139899492263794; valid loss: 2.4668221473693848\n",
      "Epoch 1000 => train loss: 0.9956650733947754; valid loss: 2.4329912662506104\n",
      "Running time in seconds: 75.82873630523682\n",
      "Scaled valid loss values are 1.15, 0.19, 0.38, 0.74, 0.38, 0.17 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 159.41970825195312; valid loss: 122.25191497802734\n",
      "Epoch 200 => train loss: 2.8676159381866455; valid loss: 5.844749450683594\n",
      "Epoch 400 => train loss: 1.6368452310562134; valid loss: 3.2027509212493896\n",
      "Epoch 600 => train loss: 1.2993340492248535; valid loss: 2.2073917388916016\n",
      "Epoch 800 => train loss: 1.0236343145370483; valid loss: 3.0896472930908203\n",
      "Running time in seconds: 66.47880125045776\n",
      "Scaled valid loss values are 1.50, 0.20, 0.52, 1.08, 0.38, 0.18 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 188.28485107421875; valid loss: 149.66725158691406\n",
      "Epoch 200 => train loss: 3.0301132202148438; valid loss: 5.312651634216309\n",
      "Epoch 400 => train loss: 1.6179625988006592; valid loss: 2.7555134296417236\n",
      "Epoch 600 => train loss: 1.1624990701675415; valid loss: 3.2631325721740723\n",
      "Epoch 800 => train loss: 1.1214807033538818; valid loss: 2.944772243499756\n",
      "Running time in seconds: 67.6234040260315\n",
      "Scaled valid loss values are 1.41, 0.20, 0.47, 1.47, 0.29, 0.24 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 186.89004516601562; valid loss: 141.3328399658203\n",
      "Epoch 200 => train loss: 3.078251838684082; valid loss: 7.327394485473633\n",
      "Epoch 400 => train loss: 1.845795750617981; valid loss: 3.118286609649658\n",
      "Epoch 600 => train loss: 1.4275975227355957; valid loss: 2.3265438079833984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800 => train loss: 1.2489112615585327; valid loss: 2.2463417053222656\n",
      "Running time in seconds: 67.7632007598877\n",
      "Scaled valid loss values are 1.13, 0.21, 0.34, 0.84, 0.29, 0.17 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 204.10366821289062; valid loss: 168.38137817382812\n",
      "Epoch 200 => train loss: 3.114236831665039; valid loss: 7.085211753845215\n",
      "Epoch 400 => train loss: 1.7383211851119995; valid loss: 3.0421793460845947\n",
      "Epoch 600 => train loss: 1.3403851985931396; valid loss: 2.529179811477661\n",
      "Epoch 800 => train loss: 1.0931732654571533; valid loss: 2.4808335304260254\n",
      "Running time in seconds: 56.05135631561279\n",
      "Scaled valid loss values are 1.09, 0.19, 0.41, 1.22, 0.41, 0.19 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 179.63363647460938; valid loss: 143.80938720703125\n",
      "Epoch 200 => train loss: 3.125551700592041; valid loss: 6.511754035949707\n",
      "Epoch 400 => train loss: 1.7258081436157227; valid loss: 3.1463990211486816\n",
      "Epoch 600 => train loss: 1.315492868423462; valid loss: 2.9303548336029053\n",
      "Epoch 800 => train loss: 1.1212102174758911; valid loss: 3.3833494186401367\n",
      "Running time in seconds: 48.09065341949463\n",
      "Scaled valid loss values are 1.46, 0.21, 0.57, 1.68, 0.41, 0.22 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 143.4292449951172; valid loss: 115.60143280029297\n",
      "Epoch 200 => train loss: 3.1815083026885986; valid loss: 5.370849132537842\n",
      "Epoch 400 => train loss: 1.9186216592788696; valid loss: 2.7711377143859863\n",
      "Epoch 600 => train loss: 1.3856474161148071; valid loss: 2.5645933151245117\n",
      "Epoch 800 => train loss: 1.1206176280975342; valid loss: 2.5986413955688477\n",
      "Running time in seconds: 68.45540976524353\n",
      "Scaled valid loss values are 0.95, 0.19, 0.68, 0.79, 0.32, 0.23 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 190.43438720703125; valid loss: 155.8352813720703\n",
      "Epoch 200 => train loss: 3.103261709213257; valid loss: 6.897510051727295\n",
      "Epoch 400 => train loss: 1.839883804321289; valid loss: 3.497487783432007\n",
      "Epoch 600 => train loss: 1.3434199094772339; valid loss: 2.4579217433929443\n",
      "Epoch 800 => train loss: 1.145754337310791; valid loss: 2.578169584274292\n",
      "Running time in seconds: 68.8774745464325\n",
      "Scaled valid loss values are 0.97, 0.19, 0.41, 0.95, 0.40, 0.17 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 188.09884643554688; valid loss: 145.35987854003906\n",
      "Epoch 200 => train loss: 3.2141366004943848; valid loss: 6.448585033416748\n",
      "Epoch 400 => train loss: 2.0672030448913574; valid loss: 3.2310879230499268\n",
      "Epoch 600 => train loss: 1.367103099822998; valid loss: 2.5171151161193848\n",
      "Epoch 800 => train loss: 1.1715351343154907; valid loss: 2.770127534866333\n",
      "Epoch 1000 => train loss: 1.0064787864685059; valid loss: 2.3316142559051514\n",
      "Running time in seconds: 77.97137498855591\n",
      "Scaled valid loss values are 1.48, 0.19, 0.30, 0.91, 0.35, 0.22 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 190.4489288330078; valid loss: 153.24620056152344\n",
      "Epoch 200 => train loss: 3.1468708515167236; valid loss: 7.135660648345947\n",
      "Epoch 400 => train loss: 1.8773072957992554; valid loss: 2.9185874462127686\n",
      "Epoch 600 => train loss: 1.3108997344970703; valid loss: 2.686710834503174\n",
      "Epoch 800 => train loss: 1.1138373613357544; valid loss: 2.419830322265625\n",
      "Epoch 1000 => train loss: 1.0230841636657715; valid loss: 2.372220754623413\n",
      "Running time in seconds: 77.71769309043884\n",
      "Scaled valid loss values are 0.88, 0.20, 0.32, 0.86, 0.35, 0.18 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 200.82278442382812; valid loss: 164.2757568359375\n",
      "Epoch 200 => train loss: 3.3133161067962646; valid loss: 6.918443202972412\n",
      "Epoch 400 => train loss: 1.943725824356079; valid loss: 3.30886173248291\n",
      "Epoch 600 => train loss: 1.3552006483078003; valid loss: 2.702263355255127\n",
      "Epoch 800 => train loss: 1.0897889137268066; valid loss: 2.8206207752227783\n",
      "Running time in seconds: 66.65678811073303\n",
      "Scaled valid loss values are 1.00, 0.20, 0.67, 1.03, 0.21, 0.21 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 189.83245849609375; valid loss: 148.13465881347656\n",
      "Epoch 200 => train loss: 3.2023260593414307; valid loss: 7.151303291320801\n",
      "Epoch 400 => train loss: 1.6886049509048462; valid loss: 3.4547383785247803\n",
      "Epoch 600 => train loss: 1.2820427417755127; valid loss: 2.6368465423583984\n",
      "Epoch 800 => train loss: 1.1482207775115967; valid loss: 3.046142339706421\n",
      "Running time in seconds: 62.29829001426697\n",
      "Scaled valid loss values are 1.40, 0.21, 0.42, 0.93, 0.52, 0.22 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 195.77786254882812; valid loss: 150.78451538085938\n",
      "Epoch 200 => train loss: 2.9118447303771973; valid loss: 6.180532932281494\n",
      "Epoch 400 => train loss: 1.8085724115371704; valid loss: 2.6494815349578857\n",
      "Epoch 600 => train loss: 1.2979329824447632; valid loss: 2.378593921661377\n",
      "Epoch 800 => train loss: 1.0397615432739258; valid loss: 2.2434911727905273\n",
      "Epoch 1000 => train loss: 0.9347536563873291; valid loss: 2.590766668319702\n",
      "Running time in seconds: 72.88850045204163\n",
      "Scaled valid loss values are 0.82, 0.18, 0.37, 0.68, 0.29, 0.13 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 180.126708984375; valid loss: 138.6535186767578\n",
      "Epoch 200 => train loss: 3.09494686126709; valid loss: 6.3221235275268555\n",
      "Epoch 400 => train loss: 1.7877097129821777; valid loss: 2.920393943786621\n",
      "Epoch 600 => train loss: 1.3199994564056396; valid loss: 2.223375082015991\n",
      "Epoch 800 => train loss: 1.165160894393921; valid loss: 2.2924070358276367\n",
      "Running time in seconds: 63.07692050933838\n",
      "Scaled valid loss values are 0.99, 0.21, 0.61, 0.57, 0.36, 0.20 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 168.19754028320312; valid loss: 128.1126708984375\n",
      "Epoch 200 => train loss: 3.1532440185546875; valid loss: 6.195734977722168\n",
      "Epoch 400 => train loss: 2.0027575492858887; valid loss: 2.813955783843994\n",
      "Epoch 600 => train loss: 1.3927042484283447; valid loss: 2.6843206882476807\n",
      "Epoch 800 => train loss: 1.1319788694381714; valid loss: 2.6664910316467285\n",
      "Running time in seconds: 63.96828508377075\n",
      "Scaled valid loss values are 1.33, 0.19, 0.51, 0.91, 0.26, 0.23 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 220.6737823486328; valid loss: 174.88258361816406\n",
      "Epoch 200 => train loss: 3.271359443664551; valid loss: 6.843235969543457\n",
      "Epoch 400 => train loss: 1.7566092014312744; valid loss: 3.104203939437866\n",
      "Epoch 600 => train loss: 1.2999498844146729; valid loss: 2.4397127628326416\n",
      "Epoch 800 => train loss: 1.03495454788208; valid loss: 2.49299693107605\n",
      "Running time in seconds: 62.82231402397156\n",
      "Scaled valid loss values are 1.53, 0.20, 0.32, 0.59, 0.43, 0.21 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 186.37242126464844; valid loss: 142.9405059814453\n",
      "Epoch 200 => train loss: 3.1959009170532227; valid loss: 7.674580097198486\n",
      "Epoch 400 => train loss: 1.9115450382232666; valid loss: 2.966668128967285\n",
      "Epoch 600 => train loss: 1.346643090248108; valid loss: 2.300955057144165\n",
      "Epoch 800 => train loss: 1.0451531410217285; valid loss: 2.535566806793213\n",
      "Epoch 1000 => train loss: 0.9843707084655762; valid loss: 2.957930564880371\n",
      "Running time in seconds: 67.87461018562317\n",
      "Scaled valid loss values are 1.06, 0.20, 0.42, 1.27, 0.27, 0.15 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 174.00453186035156; valid loss: 137.93447875976562\n",
      "Epoch 200 => train loss: 3.316345691680908; valid loss: 6.436476230621338\n",
      "Epoch 400 => train loss: 1.8382734060287476; valid loss: 2.7930045127868652\n",
      "Epoch 600 => train loss: 1.389940619468689; valid loss: 2.1456401348114014\n",
      "Epoch 800 => train loss: 1.0977184772491455; valid loss: 2.1264138221740723\n",
      "Epoch 1000 => train loss: 1.0170633792877197; valid loss: 2.2705800533294678\n",
      "Running time in seconds: 78.9820864200592\n",
      "Scaled valid loss values are 1.40, 0.20, 0.50, 0.84, 0.19, 0.21 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 190.8626251220703; valid loss: 145.4753875732422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200 => train loss: 2.8236703872680664; valid loss: 7.007045269012451\n",
      "Epoch 400 => train loss: 1.6588975191116333; valid loss: 2.489772319793701\n",
      "Epoch 600 => train loss: 1.3176751136779785; valid loss: 2.477118968963623\n",
      "Epoch 800 => train loss: 1.0527877807617188; valid loss: 2.826288938522339\n",
      "Epoch 1000 => train loss: 0.9391676187515259; valid loss: 2.8953139781951904\n",
      "Running time in seconds: 69.11691784858704\n",
      "Scaled valid loss values are 1.55, 0.18, 0.43, 1.01, 0.34, 0.19 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 174.53689575195312; valid loss: 133.43667602539062\n",
      "Epoch 200 => train loss: 3.1906118392944336; valid loss: 7.597738742828369\n",
      "Epoch 400 => train loss: 1.7871469259262085; valid loss: 2.7902965545654297\n",
      "Epoch 600 => train loss: 1.354508638381958; valid loss: 2.6234171390533447\n",
      "Epoch 800 => train loss: 1.134007215499878; valid loss: 2.8107120990753174\n",
      "Running time in seconds: 58.16548442840576\n",
      "Scaled valid loss values are 1.35, 0.22, 0.44, 1.08, 0.31, 0.26 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 154.3064727783203; valid loss: 121.55137634277344\n",
      "Epoch 200 => train loss: 2.924372434616089; valid loss: 5.9227776527404785\n",
      "Epoch 400 => train loss: 1.6699103116989136; valid loss: 2.8815159797668457\n",
      "Epoch 600 => train loss: 1.3014689683914185; valid loss: 2.391814708709717\n",
      "Epoch 800 => train loss: 1.120672345161438; valid loss: 2.703758716583252\n",
      "Epoch 1000 => train loss: 0.9890871047973633; valid loss: 2.242772340774536\n",
      "Running time in seconds: 67.93722152709961\n",
      "Scaled valid loss values are 1.25, 0.18, 0.42, 0.97, 0.33, 0.20 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 174.8355255126953; valid loss: 144.16323852539062\n",
      "Epoch 200 => train loss: 3.445396900177002; valid loss: 6.318183422088623\n",
      "Epoch 400 => train loss: 1.8839077949523926; valid loss: 2.8919332027435303\n",
      "Epoch 600 => train loss: 1.4224910736083984; valid loss: 2.6225240230560303\n",
      "Epoch 800 => train loss: 1.1566979885101318; valid loss: 2.484520435333252\n",
      "Epoch 1000 => train loss: 1.041272521018982; valid loss: 2.6488349437713623\n",
      "Running time in seconds: 70.81674408912659\n",
      "Scaled valid loss values are 1.37, 0.19, 0.48, 0.79, 0.15, 0.19 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 182.5888671875; valid loss: 140.8651123046875\n",
      "Epoch 200 => train loss: 3.1842660903930664; valid loss: 6.2089033126831055\n",
      "Epoch 400 => train loss: 1.7087161540985107; valid loss: 3.627906560897827\n",
      "Epoch 600 => train loss: 1.3340750932693481; valid loss: 2.997767686843872\n",
      "Epoch 800 => train loss: 1.162653923034668; valid loss: 2.84429669380188\n",
      "Epoch 1000 => train loss: 0.9118269681930542; valid loss: 2.967808485031128\n",
      "Running time in seconds: 68.17433977127075\n",
      "Scaled valid loss values are 1.36, 0.20, 0.32, 1.21, 0.37, 0.20 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 159.5968780517578; valid loss: 122.31951141357422\n",
      "Epoch 200 => train loss: 2.8670432567596436; valid loss: 5.851620197296143\n",
      "Epoch 400 => train loss: 1.5704245567321777; valid loss: 3.092581272125244\n",
      "Epoch 600 => train loss: 1.3813960552215576; valid loss: 2.831509590148926\n",
      "Epoch 800 => train loss: 1.1018182039260864; valid loss: 2.69156813621521\n",
      "Running time in seconds: 58.62030863761902\n",
      "Scaled valid loss values are 1.40, 0.20, 0.42, 1.11, 0.35, 0.16 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 210.48284912109375; valid loss: 165.0045623779297\n",
      "Epoch 200 => train loss: 3.3140151500701904; valid loss: 7.234797954559326\n",
      "Epoch 400 => train loss: 1.839165210723877; valid loss: 3.586625814437866\n",
      "Epoch 600 => train loss: 1.3379428386688232; valid loss: 2.5690174102783203\n",
      "Epoch 800 => train loss: 1.0439304113388062; valid loss: 2.7641444206237793\n",
      "Epoch 1000 => train loss: 0.9558506608009338; valid loss: 2.96201753616333\n",
      "Running time in seconds: 67.62851071357727\n",
      "Scaled valid loss values are 1.42, 0.17, 0.29, 0.96, 0.17, 0.23 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 155.62933349609375; valid loss: 127.4034423828125\n",
      "Epoch 200 => train loss: 3.1255009174346924; valid loss: 6.385266304016113\n",
      "Epoch 400 => train loss: 1.7330338954925537; valid loss: 2.6919147968292236\n",
      "Epoch 600 => train loss: 1.296060562133789; valid loss: 2.5611824989318848\n",
      "Epoch 800 => train loss: 1.1332226991653442; valid loss: 2.1051862239837646\n",
      "Epoch 1000 => train loss: 0.9721748232841492; valid loss: 2.2675621509552\n",
      "Epoch 1200 => train loss: 1.019803762435913; valid loss: 2.412257671356201\n",
      "Running time in seconds: 79.82328224182129\n",
      "Scaled valid loss values are 0.70, 0.18, 0.44, 1.02, 0.25, 0.19 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 204.80911254882812; valid loss: 167.8742218017578\n",
      "Epoch 200 => train loss: 3.1017422676086426; valid loss: 5.926473617553711\n",
      "Epoch 400 => train loss: 1.7160438299179077; valid loss: 2.9795002937316895\n",
      "Epoch 600 => train loss: 1.2675666809082031; valid loss: 2.6233882904052734\n",
      "Epoch 800 => train loss: 1.096282958984375; valid loss: 2.6250767707824707\n",
      "Running time in seconds: 60.82954239845276\n",
      "Scaled valid loss values are 1.06, 0.19, 0.30, 1.05, 0.54, 0.16 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 196.5364227294922; valid loss: 153.9277801513672\n",
      "Epoch 200 => train loss: 3.0510518550872803; valid loss: 6.424650192260742\n",
      "Epoch 400 => train loss: 1.7394869327545166; valid loss: 3.5152132511138916\n",
      "Epoch 600 => train loss: 1.304927110671997; valid loss: 2.9404549598693848\n",
      "Epoch 800 => train loss: 1.0809853076934814; valid loss: 2.5998756885528564\n",
      "Epoch 1000 => train loss: 0.9480551481246948; valid loss: 2.646435022354126\n",
      "Running time in seconds: 65.01544213294983\n",
      "Scaled valid loss values are 1.95, 0.20, 0.45, 0.87, 0.23, 0.29 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 151.24539184570312; valid loss: 117.15528869628906\n",
      "Epoch 200 => train loss: 2.880079746246338; valid loss: 5.7588653564453125\n",
      "Epoch 400 => train loss: 1.7958675622940063; valid loss: 2.5848987102508545\n",
      "Epoch 600 => train loss: 1.2464231252670288; valid loss: 2.2929539680480957\n",
      "Epoch 800 => train loss: 1.069076657295227; valid loss: 2.611557960510254\n",
      "Running time in seconds: 61.42332172393799\n",
      "Scaled valid loss values are 1.20, 0.20, 0.48, 1.09, 0.35, 0.18 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 165.58267211914062; valid loss: 133.03932189941406\n",
      "Epoch 200 => train loss: 3.147613286972046; valid loss: 6.131350040435791\n",
      "Epoch 400 => train loss: 1.61385977268219; valid loss: 2.741023302078247\n",
      "Epoch 600 => train loss: 1.3693785667419434; valid loss: 2.5651559829711914\n",
      "Epoch 800 => train loss: 1.1774942874908447; valid loss: 2.2426908016204834\n",
      "Epoch 1000 => train loss: 0.9952037930488586; valid loss: 2.538999557495117\n",
      "Epoch 1200 => train loss: 0.8614147305488586; valid loss: 2.2540478706359863\n",
      "Running time in seconds: 82.66959500312805\n",
      "Scaled valid loss values are 1.05, 0.19, 0.38, 0.92, 0.30, 0.22 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 216.19259643554688; valid loss: 172.91665649414062\n",
      "Epoch 200 => train loss: 3.2966177463531494; valid loss: 6.461803436279297\n",
      "Epoch 400 => train loss: 1.9265069961547852; valid loss: 3.239680290222168\n",
      "Epoch 600 => train loss: 1.432297945022583; valid loss: 2.2426669597625732\n",
      "Epoch 800 => train loss: 1.224959373474121; valid loss: 2.322289228439331\n",
      "Epoch 1000 => train loss: 1.0923665761947632; valid loss: 2.7311196327209473\n",
      "Running time in seconds: 77.48568177223206\n",
      "Scaled valid loss values are 1.07, 0.20, 0.42, 0.86, 0.37, 0.20 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 185.05032348632812; valid loss: 138.99586486816406\n",
      "Epoch 200 => train loss: 3.1201016902923584; valid loss: 6.4835357666015625\n",
      "Epoch 400 => train loss: 1.9690738916397095; valid loss: 2.990363836288452\n",
      "Epoch 600 => train loss: 1.3542526960372925; valid loss: 2.304290771484375\n",
      "Epoch 800 => train loss: 1.1726510524749756; valid loss: 2.358311891555786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 => train loss: 0.9857513308525085; valid loss: 2.5318613052368164\n",
      "Running time in seconds: 78.0957281589508\n",
      "Scaled valid loss values are 1.00, 0.19, 0.50, 0.80, 0.42, 0.18 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 177.2603759765625; valid loss: 143.38511657714844\n",
      "Epoch 200 => train loss: 3.2453818321228027; valid loss: 6.794220924377441\n",
      "Epoch 400 => train loss: 1.9397153854370117; valid loss: 3.1018683910369873\n",
      "Epoch 600 => train loss: 1.3726730346679688; valid loss: 2.4397482872009277\n",
      "Epoch 800 => train loss: 1.1252949237823486; valid loss: 2.8121018409729004\n",
      "Epoch 1000 => train loss: 1.0178744792938232; valid loss: 2.6060025691986084\n",
      "Running time in seconds: 74.64626741409302\n",
      "Scaled valid loss values are 1.34, 0.19, 0.51, 1.02, 0.37, 0.26 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 192.11904907226562; valid loss: 141.02743530273438\n",
      "Epoch 200 => train loss: 3.1923468112945557; valid loss: 6.452634811401367\n",
      "Epoch 400 => train loss: 1.6754883527755737; valid loss: 2.8967270851135254\n",
      "Epoch 600 => train loss: 1.3495616912841797; valid loss: 2.538513422012329\n",
      "Epoch 800 => train loss: 1.1037931442260742; valid loss: 2.3487744331359863\n",
      "Running time in seconds: 55.848891496658325\n",
      "Scaled valid loss values are 0.87, 0.20, 0.37, 0.92, 0.21, 0.16 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 169.36117553710938; valid loss: 119.85025787353516\n",
      "Epoch 200 => train loss: 3.143570899963379; valid loss: 6.516305923461914\n",
      "Epoch 400 => train loss: 1.7542804479599; valid loss: 2.8409011363983154\n",
      "Epoch 600 => train loss: 1.3120567798614502; valid loss: 2.897512197494507\n",
      "Epoch 800 => train loss: 1.115077018737793; valid loss: 2.7841122150421143\n",
      "Running time in seconds: 58.52098608016968\n",
      "Scaled valid loss values are 1.37, 0.20, 0.45, 1.02, 0.37, 0.21 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 179.09375; valid loss: 143.34857177734375\n",
      "Epoch 200 => train loss: 3.500267505645752; valid loss: 7.231790065765381\n",
      "Epoch 400 => train loss: 2.241959810256958; valid loss: 4.303072929382324\n",
      "Epoch 600 => train loss: 1.5317569971084595; valid loss: 2.4649741649627686\n",
      "Epoch 800 => train loss: 1.2805207967758179; valid loss: 2.6205625534057617\n",
      "Epoch 1000 => train loss: 1.0357576608657837; valid loss: 2.4220783710479736\n",
      "Running time in seconds: 71.15214586257935\n",
      "Scaled valid loss values are 1.16, 0.22, 0.37, 0.93, 0.20, 0.21 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 159.88406372070312; valid loss: 121.81640625\n",
      "Epoch 200 => train loss: 3.3756320476531982; valid loss: 6.13967752456665\n",
      "Epoch 400 => train loss: 1.9696259498596191; valid loss: 2.8870644569396973\n",
      "Epoch 600 => train loss: 1.4009289741516113; valid loss: 2.4319937229156494\n",
      "Epoch 800 => train loss: 1.1191723346710205; valid loss: 2.1866323947906494\n",
      "Epoch 1000 => train loss: 1.0833773612976074; valid loss: 2.248182773590088\n",
      "Running time in seconds: 71.07396388053894\n",
      "Scaled valid loss values are 1.05, 0.17, 0.40, 0.69, 0.20, 0.22 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 219.43487548828125; valid loss: 180.99490356445312\n",
      "Epoch 200 => train loss: 3.121161699295044; valid loss: 6.839963436126709\n",
      "Epoch 400 => train loss: 1.7859658002853394; valid loss: 3.2005953788757324\n",
      "Epoch 600 => train loss: 1.293730616569519; valid loss: 2.472029447555542\n",
      "Epoch 800 => train loss: 1.1185919046401978; valid loss: 2.550934076309204\n",
      "Epoch 1000 => train loss: 0.9672001004219055; valid loss: 2.6104166507720947\n",
      "Epoch 1200 => train loss: 0.9091342091560364; valid loss: 2.615694046020508\n",
      "Running time in seconds: 89.52848505973816\n",
      "Scaled valid loss values are 1.28, 0.22, 0.50, 0.74, 0.33, 0.19 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 187.53504943847656; valid loss: 155.58700561523438\n",
      "Epoch 200 => train loss: 3.056532382965088; valid loss: 6.8024516105651855\n",
      "Epoch 400 => train loss: 1.6495814323425293; valid loss: 3.3900651931762695\n",
      "Epoch 600 => train loss: 1.3006293773651123; valid loss: 2.8716912269592285\n",
      "Epoch 800 => train loss: 1.0941823720932007; valid loss: 2.962165594100952\n",
      "Running time in seconds: 65.68756151199341\n",
      "Scaled valid loss values are 1.34, 0.20, 0.35, 1.24, 0.33, 0.20 for Tg, Raman, density, entropy, ri, viscosity (AG)\n",
      "Full training.\n",
      "\n",
      "Epoch 0 => train loss: 206.765869140625; valid loss: 161.64175415039062\n",
      "Epoch 200 => train loss: 3.1801252365112305; valid loss: 7.399419784545898\n",
      "Epoch 400 => train loss: 1.8895477056503296; valid loss: 3.0610783100128174\n",
      "Epoch 600 => train loss: 1.4529379606246948; valid loss: 2.8097431659698486\n",
      "Epoch 800 => train loss: 1.1672534942626953; valid loss: 2.622749090194702\n",
      "Epoch 1000 => train loss: 1.068822979927063; valid loss: 2.4527862071990967\n",
      "Epoch 1200 => train loss: 0.9800513982772827; valid loss: 2.3057103157043457\n",
      "Epoch 1400 => train loss: 0.8349289894104004; valid loss: 2.8397891521453857\n",
      "Running time in seconds: 91.39634704589844\n",
      "Scaled valid loss values are 1.00, 0.20, 0.40, 1.06, 0.25, 0.21 for Tg, Raman, density, entropy, ri, viscosity (AG)\n"
     ]
    }
   ],
   "source": [
    "nb_layers = 4\n",
    "nb_neurons = 300\n",
    "p_drop = 0.01\n",
    "\n",
    "nb_exp = 50\n",
    "\n",
    "for i in range(nb_exp):\n",
    "    \n",
    "    name = \"./model/candidates/l\"+str(nb_layers)+\"_n\"+str(nb_neurons)+\"_p\"+str(p_drop)+\"_m\"+str(i)+\".pth\"\n",
    "\n",
    "    # declaring model\n",
    "    neuralmodel = imelt.model(4,nb_neurons,nb_layers,ds.nb_channels_raman,p_drop=p_drop) \n",
    "\n",
    "    # criterion for match\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')\n",
    "    criterion.to(device) # sending criterion on device\n",
    "\n",
    "    # we initialize the output bias and send the neural net on device\n",
    "    neuralmodel.output_bias_init()\n",
    "    neuralmodel = neuralmodel.float()\n",
    "    neuralmodel.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(neuralmodel.parameters(), lr = 0.0006, weight_decay=0.00) # optimizer\n",
    "    neuralmodel, record_train_loss, record_valid_loss = imelt.training(neuralmodel,ds,\n",
    "                                                                         criterion,optimizer,save_switch=True,save_name=name,\n",
    "                                                                         train_patience=400,min_delta=0.05,\n",
    "                                                                         verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect and save the best models\n",
    "\n",
    "For that we use the global \"loss_valid\" = loss_viscosity + loss_raman + loss_density + loss_refractiveindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling coefficients for loss function\n",
    "# viscosity is always one\n",
    "# scaling coefficients for loss function\n",
    "# viscosity is always one\n",
    "ls = imelt.loss_scales()        \n",
    "entro_scale = ls.entro\n",
    "raman_scale = ls.raman\n",
    "density_scale = ls.density\n",
    "ri_scale = ls.ri\n",
    "tg_scale = ls.tg\n",
    "    \n",
    "record_loss = pd.DataFrame()\n",
    "\n",
    "record_loss[\"name\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"nb_layers\"] = np.zeros(nb_exp)\n",
    "record_loss[\"nb_neurons\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_ag_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_ag_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_am_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_am_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_myega_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_myega_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_cg_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_cg_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_tvf_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_tvf_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_Sconf_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_Sconf_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_d_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_d_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_raman_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_raman_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "record_loss[\"loss_train\"] = np.zeros(nb_exp)\n",
    "record_loss[\"loss_valid\"] = np.zeros(nb_exp)\n",
    "\n",
    "for i in range(nb_exp):\n",
    "\n",
    "    name = \"./model/candidates/l\"+str(nb_layers)+\"_n\"+str(nb_neurons)+\"_p\"+str(p_drop)+\"_m\"+str(i)+\".pth\"\n",
    "    record_loss.loc[i,\"name\"] = \"l\"+str(nb_layers)+\"_n\"+str(nb_neurons)+\"_p\"+str(p_drop)+\"_m\"+str(i)+\".pth\"\n",
    "    \n",
    "    # declaring model\n",
    "    neuralmodel = imelt.model(4,nb_neurons,nb_layers,ds.nb_channels_raman,p_drop=p_drop) \n",
    "    neuralmodel.load_state_dict(torch.load(name, map_location='cpu'))\n",
    "    neuralmodel.to(device)\n",
    "    neuralmodel.eval()\n",
    "    \n",
    "    # PREDICTIONS\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # train\n",
    "        y_ag_pred_train = neuralmodel.ag(ds.x_visco_train,ds.T_visco_train)\n",
    "        y_myega_pred_train = neuralmodel.myega(ds.x_visco_train,ds.T_visco_train)\n",
    "        y_am_pred_train = neuralmodel.am(ds.x_visco_train,ds.T_visco_train)\n",
    "        y_cg_pred_train = neuralmodel.cg(ds.x_visco_train,ds.T_visco_train)\n",
    "        y_tvf_pred_train = neuralmodel.tvf(ds.x_visco_train,ds.T_visco_train)\n",
    "        y_raman_pred_train = neuralmodel.raman_pred(ds.x_raman_train)\n",
    "        y_density_pred_train = neuralmodel.density(ds.x_density_train)\n",
    "        y_entro_pred_train = neuralmodel.sctg(ds.x_entro_train)\n",
    "        y_ri_pred_train = neuralmodel.sellmeier(ds.x_ri_train, ds.lbd_ri_train)\n",
    "\n",
    "        # valid\n",
    "        y_ag_pred_valid = neuralmodel.ag(ds.x_visco_valid,ds.T_visco_valid)\n",
    "        y_myega_pred_valid = neuralmodel.myega(ds.x_visco_valid,ds.T_visco_valid)\n",
    "        y_am_pred_valid = neuralmodel.am(ds.x_visco_valid,ds.T_visco_valid)\n",
    "        y_cg_pred_valid = neuralmodel.cg(ds.x_visco_valid,ds.T_visco_valid)\n",
    "        y_tvf_pred_valid = neuralmodel.tvf(ds.x_visco_valid,ds.T_visco_valid)\n",
    "        y_raman_pred_valid = neuralmodel.raman_pred(ds.x_raman_valid)\n",
    "        y_density_pred_valid = neuralmodel.density(ds.x_density_valid)\n",
    "        y_entro_pred_valid = neuralmodel.sctg(ds.x_entro_valid)\n",
    "        y_ri_pred_valid = neuralmodel.sellmeier(ds.x_ri_valid, ds.lbd_ri_valid)\n",
    "\n",
    "        # Compute Loss\n",
    "\n",
    "        # train \n",
    "        record_loss.loc[i,\"loss_ag_train\"] = np.sqrt(criterion(y_ag_pred_train, ds.y_visco_train).item())\n",
    "        record_loss.loc[i,\"loss_myega_train\"]  = np.sqrt(criterion(y_myega_pred_train, ds.y_visco_train).item())\n",
    "        record_loss.loc[i,\"loss_am_train\"]  = np.sqrt(criterion(y_am_pred_train, ds.y_visco_train).item())\n",
    "        record_loss.loc[i,\"loss_cg_train\"]  = np.sqrt(criterion(y_cg_pred_train, ds.y_visco_train).item())\n",
    "        record_loss.loc[i,\"loss_tvf_train\"]  = np.sqrt(criterion(y_tvf_pred_train, ds.y_visco_train).item())\n",
    "        record_loss.loc[i,\"loss_raman_train\"]  = np.sqrt(criterion(y_raman_pred_train,ds.y_raman_train).item())\n",
    "        record_loss.loc[i,\"loss_d_train\"]  = np.sqrt(criterion(y_density_pred_train,ds.y_density_train).item())\n",
    "        record_loss.loc[i,\"loss_Sconf_train\"]  = np.sqrt(criterion(y_entro_pred_train,ds.y_entro_train).item())\n",
    "        record_loss.loc[i,\"loss_ri_train\"]  = np.sqrt(criterion(y_ri_pred_train,ds.y_ri_train).item())\n",
    "\n",
    "        # validation\n",
    "        record_loss.loc[i,\"loss_ag_valid\"] = np.sqrt(criterion(y_ag_pred_valid, ds.y_visco_valid).item())\n",
    "        record_loss.loc[i,\"loss_myega_valid\"] = np.sqrt(criterion(y_myega_pred_valid, ds.y_visco_valid).item())\n",
    "        record_loss.loc[i,\"loss_am_valid\"] = np.sqrt(criterion(y_am_pred_valid, ds.y_visco_valid).item())\n",
    "        record_loss.loc[i,\"loss_cg_valid\"]  = np.sqrt(criterion(y_cg_pred_train, ds.y_visco_train).item())\n",
    "        record_loss.loc[i,\"loss_tvf_valid\"]  = np.sqrt(criterion(y_tvf_pred_train, ds.y_visco_train).item())\n",
    "        record_loss.loc[i,\"loss_raman_valid\"] = np.sqrt(criterion(y_raman_pred_valid,ds.y_raman_valid).item())\n",
    "        record_loss.loc[i,\"loss_d_valid\"] = np.sqrt(criterion(y_density_pred_valid,ds.y_density_valid).item())\n",
    "        record_loss.loc[i,\"loss_Sconf_valid\"] = np.sqrt(criterion(y_entro_pred_valid,ds.y_entro_valid).item())\n",
    "        record_loss.loc[i,\"loss_ri_valid\"]  = np.sqrt(criterion(y_ri_pred_valid,ds.y_ri_valid).item())\n",
    "    \n",
    "    record_loss.loc[i,\"loss_train\"] = (record_loss.loc[i,\"loss_ag_train\"] + \n",
    "                                         record_loss.loc[i,\"loss_myega_train\"] + \n",
    "                                         record_loss.loc[i,\"loss_am_train\"] + \n",
    "                                         record_loss.loc[i,\"loss_cg_train\"] + \n",
    "                                         record_loss.loc[i,\"loss_tvf_train\"] + \n",
    "                                         raman_scale*record_loss.loc[i,\"loss_raman_train\"] + \n",
    "                                         density_scale*record_loss.loc[i,\"loss_d_train\"] + \n",
    "                                         entro_scale*record_loss.loc[i,\"loss_Sconf_train\"] + \n",
    "                                         ri_scale*record_loss.loc[i,\"loss_ri_train\"])\n",
    "    \n",
    "    record_loss.loc[i,\"loss_valid\"] = (record_loss.loc[i,\"loss_ag_valid\"] + \n",
    "                                         record_loss.loc[i,\"loss_myega_valid\"] + \n",
    "                                         record_loss.loc[i,\"loss_am_valid\"] + \n",
    "                                         record_loss.loc[i,\"loss_cg_valid\"] + \n",
    "                                         record_loss.loc[i,\"loss_tvf_valid\"] + \n",
    "                                         raman_scale*record_loss.loc[i,\"loss_raman_valid\"] + \n",
    "                                         density_scale*record_loss.loc[i,\"loss_d_valid\"] + \n",
    "                                         entro_scale*record_loss.loc[i,\"loss_Sconf_valid\"] + \n",
    "                                         ri_scale*record_loss.loc[i,\"loss_ri_valid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>nb_layers</th>\n",
       "      <th>nb_neurons</th>\n",
       "      <th>loss_ag_train</th>\n",
       "      <th>loss_ag_valid</th>\n",
       "      <th>loss_am_train</th>\n",
       "      <th>loss_am_valid</th>\n",
       "      <th>loss_myega_train</th>\n",
       "      <th>loss_myega_valid</th>\n",
       "      <th>loss_cg_train</th>\n",
       "      <th>...</th>\n",
       "      <th>loss_Sconf_train</th>\n",
       "      <th>loss_Sconf_valid</th>\n",
       "      <th>loss_d_train</th>\n",
       "      <th>loss_d_valid</th>\n",
       "      <th>loss_raman_train</th>\n",
       "      <th>loss_raman_valid</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_valid</th>\n",
       "      <th>loss_ri_train</th>\n",
       "      <th>loss_ri_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>l4_n300_p0.01_m23.pth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.280812</td>\n",
       "      <td>0.340709</td>\n",
       "      <td>0.265785</td>\n",
       "      <td>0.384762</td>\n",
       "      <td>0.293690</td>\n",
       "      <td>0.393934</td>\n",
       "      <td>0.271818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350586</td>\n",
       "      <td>0.714136</td>\n",
       "      <td>0.010922</td>\n",
       "      <td>0.016375</td>\n",
       "      <td>0.077769</td>\n",
       "      <td>0.110410</td>\n",
       "      <td>44.266288</td>\n",
       "      <td>64.686846</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.004371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>l4_n300_p0.01_m16.pth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304380</td>\n",
       "      <td>0.326652</td>\n",
       "      <td>0.285993</td>\n",
       "      <td>0.346436</td>\n",
       "      <td>0.308115</td>\n",
       "      <td>0.353811</td>\n",
       "      <td>0.280556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363064</td>\n",
       "      <td>0.827347</td>\n",
       "      <td>0.009616</td>\n",
       "      <td>0.017557</td>\n",
       "      <td>0.074919</td>\n",
       "      <td>0.102453</td>\n",
       "      <td>43.111280</td>\n",
       "      <td>65.851703</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>0.004381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l4_n300_p0.01_m1.pth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264074</td>\n",
       "      <td>0.347704</td>\n",
       "      <td>0.246733</td>\n",
       "      <td>0.399178</td>\n",
       "      <td>0.266872</td>\n",
       "      <td>0.404456</td>\n",
       "      <td>0.243739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273637</td>\n",
       "      <td>0.806578</td>\n",
       "      <td>0.008938</td>\n",
       "      <td>0.019287</td>\n",
       "      <td>0.059073</td>\n",
       "      <td>0.099099</td>\n",
       "      <td>40.108597</td>\n",
       "      <td>66.974780</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.004323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>l4_n300_p0.01_m28.pth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.270485</td>\n",
       "      <td>0.378856</td>\n",
       "      <td>0.267301</td>\n",
       "      <td>0.449257</td>\n",
       "      <td>0.276657</td>\n",
       "      <td>0.444932</td>\n",
       "      <td>0.256119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285575</td>\n",
       "      <td>0.771713</td>\n",
       "      <td>0.010982</td>\n",
       "      <td>0.014335</td>\n",
       "      <td>0.068680</td>\n",
       "      <td>0.099872</td>\n",
       "      <td>41.572669</td>\n",
       "      <td>67.593713</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>0.004868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>l4_n300_p0.01_m39.pth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.263275</td>\n",
       "      <td>0.336131</td>\n",
       "      <td>0.263845</td>\n",
       "      <td>0.387392</td>\n",
       "      <td>0.270599</td>\n",
       "      <td>0.385147</td>\n",
       "      <td>0.247386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262380</td>\n",
       "      <td>0.713704</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>0.015473</td>\n",
       "      <td>0.064299</td>\n",
       "      <td>0.102580</td>\n",
       "      <td>39.575474</td>\n",
       "      <td>67.608296</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.004775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l4_n300_p0.01_m3.pth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.280617</td>\n",
       "      <td>0.336706</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.373539</td>\n",
       "      <td>0.287584</td>\n",
       "      <td>0.373235</td>\n",
       "      <td>0.260807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300592</td>\n",
       "      <td>0.844587</td>\n",
       "      <td>0.010756</td>\n",
       "      <td>0.014670</td>\n",
       "      <td>0.070305</td>\n",
       "      <td>0.103316</td>\n",
       "      <td>41.364830</td>\n",
       "      <td>67.684986</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.004847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>l4_n300_p0.01_m42.pth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315693</td>\n",
       "      <td>0.341829</td>\n",
       "      <td>0.289548</td>\n",
       "      <td>0.379228</td>\n",
       "      <td>0.310406</td>\n",
       "      <td>0.381222</td>\n",
       "      <td>0.282576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305190</td>\n",
       "      <td>0.789517</td>\n",
       "      <td>0.009331</td>\n",
       "      <td>0.017802</td>\n",
       "      <td>0.071858</td>\n",
       "      <td>0.101451</td>\n",
       "      <td>41.049136</td>\n",
       "      <td>67.964668</td>\n",
       "      <td>0.002846</td>\n",
       "      <td>0.004564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>l4_n300_p0.01_m44.pth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.299811</td>\n",
       "      <td>0.362961</td>\n",
       "      <td>0.287771</td>\n",
       "      <td>0.402251</td>\n",
       "      <td>0.308403</td>\n",
       "      <td>0.395296</td>\n",
       "      <td>0.289364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382925</td>\n",
       "      <td>0.891941</td>\n",
       "      <td>0.009935</td>\n",
       "      <td>0.018667</td>\n",
       "      <td>0.077754</td>\n",
       "      <td>0.105466</td>\n",
       "      <td>43.258816</td>\n",
       "      <td>68.216951</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>0.004479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>l4_n300_p0.01_m33.pth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306319</td>\n",
       "      <td>0.345006</td>\n",
       "      <td>0.284438</td>\n",
       "      <td>0.388851</td>\n",
       "      <td>0.308694</td>\n",
       "      <td>0.389844</td>\n",
       "      <td>0.284104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342624</td>\n",
       "      <td>0.885360</td>\n",
       "      <td>0.009855</td>\n",
       "      <td>0.018272</td>\n",
       "      <td>0.077114</td>\n",
       "      <td>0.108276</td>\n",
       "      <td>43.820852</td>\n",
       "      <td>68.300291</td>\n",
       "      <td>0.003059</td>\n",
       "      <td>0.004526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>l4_n300_p0.01_m13.pth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276147</td>\n",
       "      <td>0.334557</td>\n",
       "      <td>0.273543</td>\n",
       "      <td>0.382873</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.387989</td>\n",
       "      <td>0.270360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332687</td>\n",
       "      <td>0.800732</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>0.017226</td>\n",
       "      <td>0.074682</td>\n",
       "      <td>0.106805</td>\n",
       "      <td>42.399042</td>\n",
       "      <td>68.570826</td>\n",
       "      <td>0.002858</td>\n",
       "      <td>0.004675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name  nb_layers  nb_neurons  loss_ag_train  \\\n",
       "23  l4_n300_p0.01_m23.pth        0.0         0.0       0.280812   \n",
       "16  l4_n300_p0.01_m16.pth        0.0         0.0       0.304380   \n",
       "1    l4_n300_p0.01_m1.pth        0.0         0.0       0.264074   \n",
       "28  l4_n300_p0.01_m28.pth        0.0         0.0       0.270485   \n",
       "39  l4_n300_p0.01_m39.pth        0.0         0.0       0.263275   \n",
       "3    l4_n300_p0.01_m3.pth        0.0         0.0       0.280617   \n",
       "42  l4_n300_p0.01_m42.pth        0.0         0.0       0.315693   \n",
       "44  l4_n300_p0.01_m44.pth        0.0         0.0       0.299811   \n",
       "33  l4_n300_p0.01_m33.pth        0.0         0.0       0.306319   \n",
       "13  l4_n300_p0.01_m13.pth        0.0         0.0       0.276147   \n",
       "\n",
       "    loss_ag_valid  loss_am_train  loss_am_valid  loss_myega_train  \\\n",
       "23       0.340709       0.265785       0.384762          0.293690   \n",
       "16       0.326652       0.285993       0.346436          0.308115   \n",
       "1        0.347704       0.246733       0.399178          0.266872   \n",
       "28       0.378856       0.267301       0.449257          0.276657   \n",
       "39       0.336131       0.263845       0.387392          0.270599   \n",
       "3        0.336706       0.271429       0.373539          0.287584   \n",
       "42       0.341829       0.289548       0.379228          0.310406   \n",
       "44       0.362961       0.287771       0.402251          0.308403   \n",
       "33       0.345006       0.284438       0.388851          0.308694   \n",
       "13       0.334557       0.273543       0.382873          0.291667   \n",
       "\n",
       "    loss_myega_valid  loss_cg_train  ...  loss_Sconf_train  loss_Sconf_valid  \\\n",
       "23          0.393934       0.271818  ...          0.350586          0.714136   \n",
       "16          0.353811       0.280556  ...          0.363064          0.827347   \n",
       "1           0.404456       0.243739  ...          0.273637          0.806578   \n",
       "28          0.444932       0.256119  ...          0.285575          0.771713   \n",
       "39          0.385147       0.247386  ...          0.262380          0.713704   \n",
       "3           0.373235       0.260807  ...          0.300592          0.844587   \n",
       "42          0.381222       0.282576  ...          0.305190          0.789517   \n",
       "44          0.395296       0.289364  ...          0.382925          0.891941   \n",
       "33          0.389844       0.284104  ...          0.342624          0.885360   \n",
       "13          0.387989       0.270360  ...          0.332687          0.800732   \n",
       "\n",
       "    loss_d_train  loss_d_valid  loss_raman_train  loss_raman_valid  \\\n",
       "23      0.010922      0.016375          0.077769          0.110410   \n",
       "16      0.009616      0.017557          0.074919          0.102453   \n",
       "1       0.008938      0.019287          0.059073          0.099099   \n",
       "28      0.010982      0.014335          0.068680          0.099872   \n",
       "39      0.009711      0.015473          0.064299          0.102580   \n",
       "3       0.010756      0.014670          0.070305          0.103316   \n",
       "42      0.009331      0.017802          0.071858          0.101451   \n",
       "44      0.009935      0.018667          0.077754          0.105466   \n",
       "33      0.009855      0.018272          0.077114          0.108276   \n",
       "13      0.010601      0.017226          0.074682          0.106805   \n",
       "\n",
       "    loss_train  loss_valid  loss_ri_train  loss_ri_valid  \n",
       "23   44.266288   64.686846       0.003003       0.004371  \n",
       "16   43.111280   65.851703       0.003016       0.004381  \n",
       "1    40.108597   66.974780       0.002842       0.004323  \n",
       "28   41.572669   67.593713       0.002758       0.004868  \n",
       "39   39.575474   67.608296       0.002700       0.004775  \n",
       "3    41.364830   67.684986       0.002752       0.004847  \n",
       "42   41.049136   67.964668       0.002846       0.004564  \n",
       "44   43.258816   68.216951       0.002989       0.004479  \n",
       "33   43.820852   68.300291       0.003059       0.004526  \n",
       "13   42.399042   68.570826       0.002858       0.004675  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_recorded = record_loss.nsmallest(10,\"loss_valid\")\n",
    "best_recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the content of \n",
    "# source to destination \n",
    "for i in best_recorded.loc[:,\"name\"]:\n",
    "    shutil.copyfile(\"./model/candidates/\"+i, \"./model/best/\"+i) \n",
    "\n",
    "best_recorded.to_csv(\"./model/best/best_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pytorch_model_dev2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
